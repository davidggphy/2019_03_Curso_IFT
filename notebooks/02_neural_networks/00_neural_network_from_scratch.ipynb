{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T00:10:07.487928Z",
     "start_time": "2019-04-01T00:10:07.480925Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to build a feed forward neural network (NN) from scratch, i.e., using numpy. This will allow us to understand the basic principles of NN, some of its problems and solutions to them.\n",
    "\n",
    "We are going to do that à la Keras, this is, using classes for each type of layer. You can think this is overkilling, but the structure of Object Orienting Programming (OOP) really fits our requirements for our task: to have objects (layers) with parameters in memory (weights), and we want to apply functions with these parameters (forward pass and predict) and also to update them (backward pass / train).\n",
    "\n",
    "A simple guide to OOP in Python 3 can be found [here](https://realpython.com/python3-object-oriented-programming/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example of a class, which is a circle, is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:04:51.072946Z",
     "start_time": "2019-03-29T17:04:51.062548Z"
    }
   },
   "outputs": [],
   "source": [
    "# In Python 3 both ways of defyning a class are equivalent\n",
    "# class Circle(object): \n",
    "class Circle():\n",
    "    # __init__ is only applied when defining an object of the class\n",
    "    def __init__(self, radius):\n",
    "        self.radius = radius\n",
    "        self.perimeter = 2* np.pi * self.radius\n",
    "    def set_radius(self, radius):\n",
    "        self.radius = radius\n",
    "        # NOTE THAT WE ARE NOT UPDATING PERIMETER\n",
    "    def area(self):\n",
    "        return np.pi*self.radius**2\n",
    "    def print_perimeter(self):\n",
    "        print('The perimeter is',self.perimeter)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:04:51.165580Z",
     "start_time": "2019-03-29T17:04:51.151477Z"
    }
   },
   "outputs": [],
   "source": [
    "# We create an object of the class Circle\n",
    "circle_obj = Circle(3)\n",
    "# Let's check what circle_obj is\n",
    "print('Check circle_obj and its type')\n",
    "print(circle_obj)\n",
    "print(type(circle_obj))\n",
    "\n",
    "# We can use its methods\n",
    "print('\\nWe use its methods, like area() and print_perimeter()')\n",
    "print(circle_obj.area())\n",
    "circle_obj.print_perimeter()\n",
    "\n",
    "# We can access to its object variables\n",
    "print('\\nWe directly access to its variables, like perimeter')\n",
    "print(circle_obj.perimeter)\n",
    "\n",
    "# We can change its variables though methods. Remember that this changes the radius, but not the perimeter\n",
    "print('\\nNow we set the object variable radius to one')\n",
    "circle_obj.set_radius(1)\n",
    "print(circle_obj.radius)\n",
    "print('But the variable perimeter has not change')\n",
    "circle_obj.print_perimeter()\n",
    "\n",
    "# Area only uses the variable radius, which has been updated\n",
    "print('\\nWe can use the method area() to print the area')\n",
    "print(circle_obj.area())\n",
    "\n",
    "# But we can direcly change the value of perimeter without changing radius\n",
    "print('\\nNow we set the value of the perimeter to -1')\n",
    "circle_obj.perimeter = -1.\n",
    "circle_obj.print_perimeter()\n",
    "print('\\nIT IS CLEAR THAT THIS IS NOT A CONSISTENT WAY FOR DEFYNING THIS CLASS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation algorithm (theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start defying the shapes of our data structures. For example, the features and targets of out dataset:\n",
    "$$ X = (M, n_0)\\;\\;\\textrm{and} \\;\\; Y = (M, n_C) \\;,$$\n",
    "where $M$ is the number of items (it does not have no coincide with the length of the dataset if we are working with batches), $n_0$ is the number of features of our inputs and $n_C$ the number of classes, since we are going to use one-hot encoding.\n",
    "\n",
    "The dense layer has a matrix of weights $W$ and the vector of biases $b$, if it has $n_0$ input features and $n_1$ output features, the shape of them is\n",
    "$$W = (n_0,n_1)\\;\\;\\textrm{and} \\;\\; b = (n_1).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Warm up: Linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T19:54:19.693776Z",
     "start_time": "2019-03-31T19:54:19.657562Z"
    }
   },
   "source": [
    "\n",
    "#### Forward\n",
    "\n",
    "The operation of a dense layer, applied to one item $X_{(a)}\\in X$ is\n",
    "$$Z_{(a)}^j= X_{(a)}^i\\cdot W^{ij} + b^j$$\n",
    "$$(n_1) = (n_0) \\cdot  (n_0,n_1)+ (n_1)\\;,$$\n",
    "and this can be directly apply to the whole batch (remember numpy's broadcasting)\n",
    "$$Z = X \\cdot W + b$$\n",
    "$$(M,n_1) = (M,n_0) \\cdot  (n_0,n_1)+ (n_1) \\;.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T02:57:06.752262Z",
     "start_time": "2019-03-25T02:57:06.698952Z"
    }
   },
   "source": [
    "\n",
    "#### Backward (warm up)\n",
    "First of all, let us assume that we have a loss function which is local in the inputs, this is\n",
    "$$\\mathcal{L}(X,Y,\\theta) = \\frac{1}{M}\\sum_{a=1}^M \\mathcal{L}(X_{(a)},Y_{(a)},\\theta)\\;,$$\n",
    "where we are denoting all the training parameters as $\\theta$.\n",
    "\n",
    "For example, for a linear regression we have\n",
    "$\\mathcal{L}(X_{(a)},Y_{(a)},\\theta) = \\frac{1}{2}(Z_{(a)}- Y_{(a)})^2$, with $Z_{(a)}= X_a\\cdot W + b$,\n",
    "and the gradients of the loss function w.r.t. the parameters are easily computed as:\n",
    "$$\\frac{\\partial \\mathcal{L}_{(a)}}{\\partial b^{j}} =\n",
    "\\frac{\\partial \\mathcal{L}_{(a)}}{\\partial Z_{(b)}^{k}} \\frac{\\partial Z_{(b)}^{k}}{\\partial b^{j}} =\n",
    "\\frac{\\partial \\mathcal{L}_{(a)}}{\\partial Z_{(b)}^{k}}\\delta_{(ab)} \\delta^{kj} =\n",
    "\\frac{\\partial \\mathcal{L}_{(a)}}{\\partial Z_{(a)}^{j}}\n",
    "\\rightarrow \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{M}\\sum_a  \\frac{\\partial \\mathcal{L}_{(a)}}{\\partial Z_{(a)}}\n",
    "\\;, $$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}_{(a)}}{\\partial W^{ij}} =\n",
    "\\frac{\\partial \\mathcal{L}_{(a)}}{\\partial Z_{(b)}^{k}} \\frac{\\partial Z_{(b)}^{k}}{\\partial W^{ij}} =\n",
    "\\frac{\\partial \\mathcal{L}_{(a)}}{\\partial Z_{(b)}^{k}} \\delta_{(ab)}  X_{(b)}^l  \\delta^{li} \\delta^{kj} \n",
    "=\n",
    "X_{(a)}^i \\frac{\\partial \\mathcal{L}_{(a)}}{\\partial Z_{(a)}^{j}} \n",
    "\\rightarrow \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = \\frac{1}{M}\\sum_a  X_{(a)} \\cdot \\frac{\\partial \\mathcal{L}_{(a)}}{\\partial Z_{(a)}}   \n",
    "\\;, $$\n",
    "\n",
    "Now, if we add an activation function (element-wise) the expressions only change slightly\n",
    "$$\\tilde{Z}_{(a)}^{j} =  g\\left( Z_{(a)}^{j} \\right)= g\\left( X_{(a)}^{i}\\cdot W^{ij} + b^{j} \\right)\\;.$$\n",
    "\n",
    "All we have to do is to apply the chain rule one step further:\n",
    "$$\\frac{\\partial \\mathcal{L}_{(a)}}{\\partial Z_{(a)}^{k}} \\rightarrow\n",
    "\\frac{\\partial \\mathcal{L}_{(a)}}{\\partial \\tilde{Z}_{(a)}^{k}} \\frac{\\partial \\tilde{Z}_{(a)}^{k} }{\\partial Z_{(a)}^{k}} = \n",
    "\\frac{\\partial \\mathcal{L}_{(a)}}{\\partial \\tilde{Z}_{(a)}^{k}}g'(Z_{(a)}^{k}) \\;,\n",
    "$$\n",
    "which, in matrix notation, change the equations above into\n",
    "$$Z_{(a)}^{j} = X_{(a)}^{i}\\cdot W^{ij} + b^{j}$$\n",
    "and the gradients of the loss function w.r.t. the parameters are easily computed as:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{M}\\sum_a  \\frac{\\partial \\mathcal{L}_{(a)}}{\\partial Z_{(a)}}\n",
    " \\ast g'(Z_{(a)}) \\;, $$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = \\frac{1}{M}\\sum_a  X_{(a)} \\cdot \\left( \\frac{\\partial \\mathcal{L}_{(a)}}{\\partial Z_{(a)}} \\ast g'(Z_{(a)})   \\right)\n",
    "\\;, $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T19:53:19.737144Z",
     "start_time": "2019-03-31T19:53:19.713332Z"
    }
   },
   "source": [
    "\n",
    "### General number of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Generalization. Forward\n",
    "\n",
    "Now, let us generalize to an arbitrary number of layers, so we will add another index $[l]$. Notice that we have three kind of indices:\n",
    "* Feature space: $i,j,k,l,\\ldots$\n",
    "* Items space: $(a,b,c,d,\\ldots)$\n",
    "* Layers space: $[l,m,\\ldots]$\n",
    "\n",
    "We are gonna denote the input features as $X_{[0]}=\\{ X_{[0](a)}^i    \\}$, since they correspond to layer zero.\n",
    "\n",
    "Instead of the usual convention, in which each forward pass consists in two operations (the affine transformation and the non-linear activation layer), we are gonna consider each operation as a separate layer.\n",
    "\n",
    "So, we are going to have two different types of layers:\n",
    "* Activation function (or nonlinearity)\n",
    "$$\n",
    "X_{(a)[l]}^j = g_{[l]}\\left(  \n",
    "X_{(a)[l-1]}^j\n",
    "\\right)\\;.\n",
    "$$\n",
    "* Dense layer\n",
    "$$X_{(a)[l]}^j = \n",
    "X_{(a)[l-1]}^i\\cdot W_{[l]}^{ij} + b_{[l]}^j  \\;.$$\n",
    "or, in matricial notation\n",
    "$$X_{[l]} =  X_{[l-1]}\\cdot W_{[l]} + b_{[l]}   \\;,$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T02:56:56.828148Z",
     "start_time": "2019-03-25T02:56:56.812114Z"
    }
   },
   "source": [
    "\n",
    "#### Backward (generalization)\n",
    "\n",
    "Our goal now is to calculate the gradient of the loss function w.r.t. all the trainable parameters, this is, the set of weights matrices $W_{[l]}$ and biases $b_{[l]}$. We are going to denote them as:\n",
    "$$\n",
    "\\delta W_{[l]} \\equiv \\frac{ \\partial  \\mathcal{L} }{\\partial W_{[l]} } \\;\\; \\textrm{and} \\;\\;\n",
    "\\delta b_{[l]} \\equiv \\frac{ \\partial  \\mathcal{L} }{\\partial b_{[l]} } \\;.\n",
    "$$\n",
    "Although they are not our final goal, the partial derivatives w.r.t. the inputs at each layer would be useful, we are also defining:\n",
    "$$\n",
    "\\delta X_{(a)[l]} \\equiv \\frac{ \\partial  \\mathcal{L} }{\\partial X_{(a)[l]} } \\; .\n",
    "$$\n",
    "\n",
    "And in this point is when the recursive structure is evident:\n",
    "$$\n",
    " \\delta X_{(a)[l-1]}^i = \\delta X_{(a)[l]}^j   \\frac{ \\partial  X_{(a)[l]}^j }{\\partial X_{(a)[l-1]}^i } \\;.\n",
    "$$\n",
    "We notice that, in order to calculate the gradients on layer $[l-1]$, we first need to compute the gradient on layer $[l]$. So, the strategy is evident, we calculate the gradient of the loss function w.r.t. the output of the NN, and **backpropagate** from them.\n",
    "\n",
    "The only part that we need to calculate is $\\frac{ \\partial  X_{(a)[l]}^j }{\\partial X_{(a)[l-1]}^i }$, that depends on the type of layer $[l]$. Let us calculate this factor for the two types of layers we are dealing with:\n",
    "* Activation function (or nonlinearity)\n",
    "$$\n",
    "X_{(a)[l]}^j = g_{[l]}\\left(  \n",
    "X_{(a)[l-1]}^j\n",
    "\\right) \n",
    "\\Rightarrow  \n",
    "\\frac{ \\partial  X_{(a)[l]}^j }{\\partial X_{(a)[l-1]}^i } = \\delta^{ij} g_{[l]}\\left(  \n",
    "X_{(a)[l-1]}^j\n",
    "\\right)  \\;.\n",
    "$$\n",
    "* Dense layer\n",
    "$$X_{(a)[l]}^j = \n",
    "X_{(a)[l-1]}^i\\cdot W_{[l]}^{ij} + b_{[l]}^j \n",
    "\\Rightarrow\n",
    "\\frac{ \\partial  X_{(a)[l]}^j }{\\partial X_{(a)[l-1]}^i } =  W_{[l]}^{ij} \n",
    "\\;.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Trainable parameters\n",
    "Now we need to connect this with the derivative of the loss function w.r.t. the trainable parameters, we only need two partial derivatives (notice that we are assuming that the activation layer doesn't have trainable parameters). Combining it with the previous calculations we have:\n",
    "$$\n",
    "\\frac{ \\partial \\mathcal{L} }{\\partial W_{[l]}^{ik} } \\equiv \\delta W_{[l]}^{ik} =\n",
    "\\sum_a  \\delta X_{(a)[l]}^j \\frac{ \\partial  X_{(a)[l]}^j }{\\partial W_{[l]}^{ik} }\n",
    "\\; ,\n",
    "$$\n",
    "and for the biases\n",
    "$$\n",
    "\\frac{ \\partial \\mathcal{L} }{\\partial b_{[l]}^{k} } \\equiv \\delta b_{[l]}^{k} =\n",
    "\\sum_a  \\delta X_{(a)[l]}^j \\frac{ \\partial  X_{(a)[l]}^j }{\\partial b_{[l]}^{k} }\n",
    "\\; ,\n",
    "$$\n",
    "\n",
    "which tells us that we only need to calculate the following partial derivatives\n",
    "$$\n",
    "\\frac{ \\partial  X_{(a)[l]}^j }{\\partial W_{[l]}^{ik} }\n",
    " \\;\\; \\textrm{and} \\;\\; \n",
    " \\frac{ \\partial  X_{(a)[l]}^j }{\\partial b_{[l]}^k }\n",
    "$$\n",
    "\n",
    "They simply are:\n",
    "$$\n",
    "\\frac{ \\partial  X_{(a)[l]}^j }{\\partial W_{[l]}^{ik} } = X_{(a)[l-1]}^i \\delta^{jk}\n",
    "$$\n",
    "\n",
    "$$\n",
    " \\frac{ \\partial  X_{(a)[l]}^j }{\\partial b_{[l]}^k } = \\delta^{jk}\n",
    "$$\n",
    "\n",
    "Then, the updating rules under Gradient Descent for the Dense layer weights and biases are:\n",
    "$$\n",
    "W_{[l]}^{ik} \\rightarrow W_{[l]}^{ik} - \\epsilon \\, \\delta W_{[l]}^{ik} \\; , \\; \n",
    "\\delta W_{[l]}^{ik} =\n",
    "\\sum_a  \\delta X_{(a)[l]}^k X_{(a)[l-1]}^i \n",
    "$$\n",
    "\n",
    "$$\n",
    " b_{[l]}^k \\rightarrow  b_{[l]}^k - \\epsilon \\, \\delta b_{[l]}^k \\; , \\; \n",
    "\\delta  b_{[l]}^k =\n",
    "\\sum_a  \\delta X_{(a)[l]}^k \n",
    "$$\n",
    "\n",
    "Together with the backpropagation of the feature gradients at every layer we have all the ingredients:\n",
    "$$\n",
    " \\delta X_{(a)[l-1]}^i = \\delta X_{(a)[l]}^j   \\frac{ \\partial  X_{(a)[l]}^j }{\\partial X_{(a)[l-1]}^i } \\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Parent class Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T16:26:22.532387Z",
     "start_time": "2019-02-27T16:26:22.528523Z"
    },
    "hidden": true
   },
   "source": [
    "Inheritance is the process by which one class takes on the attributes and methods of another. Newly formed classes are called child classes, and the classes that child classes are derived from are called parent classes.\n",
    "\n",
    "Here we define the parent class `Layer`, we are going to define the rest of the layers as child classes of this one. So this is going to be a dummy layer, which will serve as a template to build the rest of them.\n",
    "\n",
    "It’s important to note that child classes override or extend the functionality (e.g., attributes and behaviors) of parent classes. In other words, child classes inherit all of the parent’s attributes and behaviors but can also specify different behavior to follow. The most basic type of class is an object, which generally all other classes inherit as their parent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All our layers are going to have, at least, two methods: `forward` and `backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:04:52.690114Z",
     "start_time": "2019-03-29T17:04:52.683443Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    '''\n",
    "    Base layer class. This is the class from which all layers inherit.\n",
    "    \n",
    "    The descendants of Layer should implement the following methods:\n",
    "        __init__(): Initialize the parameters of the layer\n",
    "        forward(): Process input to get output\n",
    "        backward(): Propagate gradients through itself \n",
    "            and update the learnable parameters if trainable=True\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trainable: Boolean, whether the layer's variables should be trainable.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    None\n",
    "    \n",
    "    Example\n",
    "    --------\n",
    "    >>> layer = Layer()\n",
    "    >>> output = layer.forward(input)\n",
    "    >>> grad_input = layer.backward(input, grad_output)\n",
    "    ''' \n",
    "\n",
    "    def __init__(self, trainable = True):\n",
    "        \"\"\"\n",
    "        Initialize the attributes of the Neural Networks, its weights and auxiliary parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : np.array\n",
    "            input data of shape [batch, input_units]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output : np.array\n",
    "            output data of shape [batch, output_units]\n",
    "        \"\"\"\n",
    "        self.trainable = True\n",
    "        # A dummy layer does nothing\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the Neural Network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : np.array\n",
    "            input data of shape [batch, input_units]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output : np.array\n",
    "            output data of shape [batch, output_units]\n",
    "        \"\"\"\n",
    "        # A dummy layer just returns whatever it gets as input.\n",
    "        output = input\n",
    "        return output\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, with respect to the given input.\n",
    "        It computes the gradients using the chain rule:\n",
    "        output = layer (input) => (d output / d x) = (d output / d layer) * (d layer / d x)\n",
    "        If there are learnable parameters, you need to update them using d loss / d layer.\n",
    "        \n",
    "        You have already received grad_output = (d output / d layer), so you only need to \n",
    "        multiply it by (d layer / d x)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : np.array\n",
    "            input data of shape [batch, input_units]\n",
    "        grad_output : np.array\n",
    "            gradient array of the output of shape [TODO]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output : np.array\n",
    "            output data of shape [batch, output_units]\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
    "        \n",
    "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "        \n",
    "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
    "        \"\"\"\n",
    "        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n",
    "        num_units = input.shape[1]\n",
    "        d_layer_d_input = np.eye(num_units) # Identity matrix\n",
    "        grad_input = np.dot(grad_output, d_layer_d_input) # chain rule\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of the problem\n",
    "\n",
    "We're going to build a neural network for classification. To do so, we'll need a few building blocks:\n",
    "- Dense layer - a fully-connected layer.\n",
    "- ReLU layer (we will play with more non-linear functions later)\n",
    "- Loss function - crossentropy\n",
    "- Backprop algorithm - a stochastic gradient descent with backpropageted gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Activation function layer (nonlinearity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:04:53.274086Z",
     "start_time": "2019-03-29T17:04:53.268950Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \"\"\"\n",
    "    ReLU layer or Rectified Linear Unit.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.trainable = False\n",
    "        self.params = 0\n",
    "        self.train_params = 0\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input : np.array of shape=(items, input_units)\n",
    "        \"\"\"\n",
    "        return np.maximum(0,input)\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Backpropagate the input gradients through the layer.\n",
    "        In this case, there are not trainable parameters.\n",
    "        For an activation function, this is simply the derivative of the function applied \n",
    "        on the inputs.\n",
    "        \"\"\"\n",
    "        relu_grad = 1.*(input > 0)\n",
    "        return grad_output*relu_grad        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the activation function, a dense layer has parameters to update at each learning step.\n",
    "\n",
    "A dense layer applies affine transformation. In a vectorized form, it can be described as:\n",
    "$$X_{[l]}= X_{[l-1]} \\cdot W_{[l]} + b_{[l]} $$\n",
    "\n",
    "Where: \n",
    "* $X_{[l-1]}$ is an items-feature matrix of shape (batch_size, num_features)\n",
    "* $W$ is the weights matrix of shape (num_features, num_outputs)\n",
    "* $b$ is the biases vector of shape (num_outputs)\n",
    "\n",
    "Both $W$ and $b$ are initialized during layer creation and updated each time backward is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T20:42:53.997494Z",
     "start_time": "2019-03-31T20:42:53.966957Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"\n",
    "    Dense layer applying an affine transformation. \n",
    "    $$ X_{[l]}= X_{[l-1]} \\cdot W_{[l]} + b_{[l]} $$\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1, trainable = True):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trainable = trainable\n",
    "        # Initialize weights with small random numbers. We use a gaussian distribution. \n",
    "        self.weights = np.random.randn(input_units, output_units)*0.01        \n",
    "        self.biases = np.zeros(output_units)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation.\n",
    "        input : np.array of shape=(items, input_units)\n",
    "        output : np.array of shape=(items, output_units)\n",
    "        \"\"\"\n",
    "        return np.matmul(input,self.weights)+self.biases\n",
    "    \n",
    "    def backward(self,input,grad_output):\n",
    "        \"\"\"\n",
    "        Backpropagate the gradient of the features and train the parameters\n",
    "        using Gradient Descent.\n",
    "        input : np.array of shape=(items, input_units)\n",
    "        grad_output : np.array of shape=(items, output_units)\n",
    "        \"\"\"\n",
    "        grad_input = np.matmul(grad_output,self.weights.T)    \n",
    "        if self.trainable:\n",
    "            # compute gradient w.r.t. weights and biases\n",
    "            grad_weights = np.matmul(input.T,grad_output)\n",
    "            grad_biases = np.sum(grad_output,axis=0)\n",
    "            assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "            # Here we perform a stochastic gradient descent step. \n",
    "            self.weights = self.weights - self.learning_rate * grad_weights\n",
    "            self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### The loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "Since we want to predict probabilities, it would be logical for us to define softmax nonlinearity on top of our network and compute loss given predicted probabilities.\n",
    "$$ \n",
    "\\mathcal{L}_{(a)} = - \\sum_i  t_{(a)}^i \\log{p_{(a)}^i} \\, ,\n",
    "$$\n",
    "where $t_{(a)}^i$ is zero unless $i$ is the true class of item $(a)$, when it is one, the **one-hot encoding** of the class labels.\n",
    "\n",
    "If you write down the expression for crossentropy as a function of softmax logits (the features of the last layer), we have the following expression:\n",
    "$$\n",
    "p_{(a)}^i = \\frac{X_{(a)[L]}^i}{\\sum_j X_{(a)[L]}^j}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\mathcal{L}_{(a)} = - \\sum_i  t_{(a)}^i \\log{\\frac{X_{(a)[L]}^i}{\\sum_j X_{(a)[L]}^j}} \n",
    "$$\n",
    "\n",
    "This can be simplified to:\n",
    "$$ \n",
    "\\mathcal{L}_{(a)} = - \\sum_i  t_{(a)}^i\n",
    "\\left(  \n",
    "X_{(a)[L]}^i - \\log{   \\sum_j X_{(a)[L]}^j} \n",
    "\\right)\n",
    "$$\n",
    "\n",
    "It's called Log-softmax and it's better than naive log(softmax(a)) in all aspects:\n",
    "* Better numerical stability\n",
    "* Easier to get derivative right\n",
    "* Marginally faster to compute\n",
    "\n",
    "So why not just use log-softmax throughout our computation and never actually bother to estimate probabilities when training?. In case we need them for prediction, we can compute them easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:04:54.179405Z",
     "start_time": "2019-03-29T17:04:54.170601Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits,true_labels):\n",
    "    \"\"\"\n",
    "    Compute crossentropy from logits and ids of correct labels\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : np.array of rank 2\n",
    "        logits, or the features after applying the last layer, of shape (items,n_classes)\n",
    "    true_labels: np.array of rank 1, dtype = np.int\n",
    "        true labels of the items WITHOUT one-hot encoding. This is, with the integer of the class\n",
    "        E.g.: true_labels = np.array([0,3,2,4,1,2,3,4]), for a 5 classes classification.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    crossentropy : np.float\n",
    "        Cross entropy loss function\n",
    "    \"\"\"\n",
    "    # We only need the logits of the true class, since only one $ t_{(a)}^i $ contributes to the sum.\n",
    "    logits_for_true_answers = logits[np.arange(len(logits)),true_labels]\n",
    "    crossentropy = - logits_for_true_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    return crossentropy\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits,true_labels):\n",
    "    \"\"\"\n",
    "    Compute crossentropy gradients from logits and ids of correct labels\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : np.array of rank 2\n",
    "        logits, or the features after applying the last layer, of shape (items,n_classes)\n",
    "    true_labels: np.array of rank 1, dtype = np.int\n",
    "        true labels of the items WITHOUT one-hot encoding. This is, with the integer of the class\n",
    "        E.g.: true_labels = np.array([0,3,2,4,1,2,3,4]), for a 5 classes classification.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    grad_softmax : np.array of rank 2\n",
    "        Gradient matrix of cross entropy loss function. Normalized with the number of items.\n",
    "        Shape: (items, number_classes)\n",
    "    \"\"\"\n",
    "    \n",
    "    # One-hot encoding matrix of the labels:  t_{(a)}^i\n",
    "    # np.array of shape (items, number_classes), with ones in the true label, zeros otherwise.\n",
    "    one_hot_true_labels = np.zeros_like(logits)\n",
    "    one_hot_true_labels[np.arange(len(logits)),true_labels] = 1\n",
    "    \n",
    "    # We calculate the probabilities with a softmax function\n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    # Notice that we divide w.r.t. the number of items, so we are taking the mean value\n",
    "    # over all the items in a batch. \n",
    "    # During backpropagation, we only have to calculate the sum\n",
    "    # w.r.t. the index $ (a)$.\n",
    "    number_items = logits.shape[0]\n",
    "    grad_softmax = -( one_hot_true_labels - softmax) / number_items\n",
    "    return grad_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to combine all the layers to form a network, and implement the backpropagation algorithm.\n",
    "We divide it into two steps, the forward and the backward passes.\n",
    "\n",
    "In the forward pass we need to store all the feature vectors $ X_{(a)[l]}^j $ for all layers, since we will need them in the backpropagation of the gradients to update the parameters of the network.\n",
    "\n",
    "Also, we define `predict` and `predict_probs` functions to predict the labels and the probabilities of each label, respectively.\n",
    "\n",
    "##TODO## BUILD A NETWORK METACLASS CLASS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T22:46:39.892154Z",
     "start_time": "2019-03-31T22:46:39.879928Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward(network, X):\n",
    "    \"\"\"\n",
    "    Compute activations of all network layers by applying them sequentially.\n",
    "    Return a list of activations for each layer. \n",
    "    Last activation corresponds to network logits.\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    input = X\n",
    "    # Loop over the network and update the input of each layer as the output of the previous one.\n",
    "    for i,layer in enumerate(network):\n",
    "        activations.append(layer.forward(input))\n",
    "        input=activations[i]\n",
    "        \n",
    "    assert len(activations) == len(network), \"The number of layers and number of activations don't coincide. \"\n",
    "    return activations\n",
    "\n",
    "def predict(network,X):\n",
    "    \"\"\"\n",
    "    Compute network label predictions. This is, the index of the most probable class.\n",
    "    \"\"\"\n",
    "    logits = forward(network,X)[-1]\n",
    "    # Notice that we don't need to calculate the probabilities, since the max logit correspond to the max prob.\n",
    "    predicted_class = logits.argmax(axis=-1)\n",
    "    return predicted_class\n",
    "\n",
    "def predict_probs(network,X):\n",
    "    \"\"\"\n",
    "    Compute network label's probability predictions.\n",
    "    \"\"\"\n",
    "    logits = forward(network,X)[-1]\n",
    "    probs = np.exp(logits)\n",
    "    probs = probs / np.sum(probs)\n",
    "    return probs\n",
    "\n",
    "def train(network,X,y):\n",
    "    \"\"\"\n",
    "    Train your network on a given batch of X and y.\n",
    "    You first need to run forward to get all layer activations.\n",
    "    Then you can run layer.backward going from last to first layer.\n",
    "    \n",
    "    After you called backward for all layers, all Dense layers have already made one gradient step.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the layer activations\n",
    "    layer_activations = forward(network,X)\n",
    "    layer_inputs = [X]+layer_activations  #layer_input[i] is an input for network[i]\n",
    "    logits = layer_activations[-1]\n",
    "    \n",
    "    # Compute the loss and the initial gradient\n",
    "    loss = softmax_crossentropy_with_logits(logits,y)\n",
    "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
    "    \n",
    "    # propagate gradients through the network>\n",
    "    for layer,layer_input in reversed(tuple(zip(network,layer_inputs))):\n",
    "        loss_grad = layer.backward(layer_input,loss_grad)\n",
    "         \n",
    "    return np.mean(loss)\n",
    "\n",
    "def set_learning_rate(network,learning_rate):\n",
    "    '''\n",
    "    Change the learning rate of a network.\n",
    "    This is useful to have an adaptative learning rate.\n",
    "    '''\n",
    "    for layer in network:\n",
    "        if hasattr(layer,'learning_rate'): layer.learning_rate = learning_rate\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to use stochastic gradient descent, making the training easier and faster in general. In order to do that, we need to build an iterator, this is done with the `yield` command.\n",
    "\n",
    "The `tqdm` library shows progress bars during training, you can find its Github repository [here](https://github.com/tqdm/tqdm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T22:49:08.851601Z",
     "start_time": "2019-03-31T22:49:08.843520Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T22:49:09.206780Z",
     "start_time": "2019-03-31T22:49:09.202905Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a MNIST classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T21:48:20.137837Z",
     "start_time": "2019-03-31T21:48:18.108140Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "loaded = np.load('NMIST_dataset.npz')\n",
    "X = loaded['X']\n",
    "X =  X.reshape((-1,X.shape[1]*X.shape[2]))\n",
    "# IMPORTANT: CHECK WHAT HAPPENS IF WE DON'T SCALE\n",
    "X = X/255.\n",
    "y = loaded['Y']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.3)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_val,y_val,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T21:48:23.007986Z",
     "start_time": "2019-03-31T21:48:20.773786Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from preprocessed_mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T21:48:25.747558Z",
     "start_time": "2019-03-31T21:48:25.730092Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_train[36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T21:48:27.314537Z",
     "start_time": "2019-03-31T21:48:27.294182Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T10:05:30.900762Z",
     "start_time": "2019-03-27T10:05:30.896569Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Benchmark: linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Notice that training the linear model on the whole training set will take several minutes. It should give a similar accuracy training on a part of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T21:54:54.905534Z",
     "start_time": "2019-03-31T21:54:54.900555Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T21:54:55.495817Z",
     "start_time": "2019-03-31T21:54:55.492351Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T22:53:38.138493Z",
     "start_time": "2019-03-31T22:50:09.435595Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_clf_saga = LogisticRegression(solver='saga' ,multi_class='multinomial').fit(X_train,y_train)\n",
    "np.mean(log_clf_saga.predict(X_val)==y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T22:41:52.184710Z",
     "start_time": "2019-03-31T22:41:16.640709Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_clf_saga_part = LogisticRegression(solver='saga' ,multi_class='multinomial').fit(X_train[:10000],y_train[:10000])\n",
    "np.mean(log_clf_saga_part.predict(X_val)==y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network with one layer, a.k.a. linear logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:03:58.107754Z",
     "start_time": "2019-03-31T23:03:58.027154Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "network = []\n",
    "network.append(Dense(X_train.shape[1],10,learning_rate=learning_rate))\n",
    "print('Initial accuracy:',np.mean(predict(network,X_val)==y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T22:54:55.051512Z",
     "start_time": "2019-03-31T22:54:21.127230Z"
    }
   },
   "outputs": [],
   "source": [
    "train_log = []\n",
    "val_log = []\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
    "        train(network,x_batch,y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(predict(network,X_train)==y_train))\n",
    "    val_log.append(np.mean(predict(network,X_val)==y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log[-1])\n",
    "    print(\"Val accuracy:\",val_log[-1])\n",
    "    plt.plot(train_log,label='train accuracy')\n",
    "    plt.plot(val_log,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "print('Maximum accuracy on validation set:',max(val_log), 'achieved at epoch:',np.argmax(val_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, we get a similar accuracy $(\\sim 93\\%)$ as with the Logistic Regression classifier of sklearn, but this has been faster, since we are using stochatic gradient descent (SGD).\n",
    "\n",
    "Indeed, the model is exactly the same, a linear logistic regression with a crossentropy loss (maximum likelihood of the multinomial distribution).\n",
    "\n",
    "As you can see, there is no overfitting applying this model, the accuracies for the training and validation sets are very close to each other.\n",
    "\n",
    "You can play with the number of `epochs` and the `learning_rate`, and see the influence, especially of the later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network with 3 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without training we expect a random prediction, giving an accuracy of 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:04:40.519028Z",
     "start_time": "2019-03-31T23:04:40.439308Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "network = []\n",
    "network.append(Dense(X_train.shape[1],50,learning_rate=learning_rate))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(50,20,learning_rate=learning_rate))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(20,10,learning_rate=learning_rate))\n",
    "print('Initial accuracy:',np.mean(predict(network,X_val)==y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:06:08.729380Z",
     "start_time": "2019-03-31T23:04:41.124673Z"
    }
   },
   "outputs": [],
   "source": [
    "train_log = []\n",
    "val_log = []\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
    "        train(network,x_batch,y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(predict(network,X_train)==y_train))\n",
    "    val_log.append(np.mean(predict(network,X_val)==y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log[-1])\n",
    "    print(\"Val accuracy:\",val_log[-1])\n",
    "    plt.plot(train_log,label='train accuracy')\n",
    "    plt.plot(val_log,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "print('Maximum accuracy on validation set:',max(val_log), 'achieved at epoch:',np.argmax(val_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with a more complex NN, we achieve $97\\%$ accuracy. The training pattern is really different in this case, once we have achieved epoch 17, the validation accuracy saturates, but the training one keeps growing. You can see that there is an overfitting problem, but also that the severity of it depends on the number of epochs.\n",
    "\n",
    "Usually, you would choose the model trained at the epoch in which the validation loss stops decreasing (or, in this case, the validation accuracy stops increasing) as a way of regularizing your model. This is an example of the so called [early stopping](https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/).\n",
    "\n",
    "If we increase the number of parameters, the overfitting should be more noticeable. Although here the accuracy on the training set is close to its maximum $(100\\%)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:16:28.595492Z",
     "start_time": "2019-03-31T23:16:28.419103Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "network = []\n",
    "network.append(Dense(X_train.shape[1],128,learning_rate=learning_rate))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(128,64,learning_rate=learning_rate))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(64,10,learning_rate=learning_rate))\n",
    "print('Initial accuracy:',np.mean(predict(network,X_val)==y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:18:06.429316Z",
     "start_time": "2019-03-31T23:16:30.469374Z"
    }
   },
   "outputs": [],
   "source": [
    "train_log = []\n",
    "val_log = []\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
    "        train(network,x_batch,y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(predict(network,X_train)==y_train))\n",
    "    val_log.append(np.mean(predict(network,X_val)==y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log[-1])\n",
    "    print(\"Val accuracy:\",val_log[-1])\n",
    "    plt.plot(train_log,label='train accuracy')\n",
    "    plt.plot(val_log,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "print('Maximum accuracy on validation set:',max(val_log), 'achieved at epoch:',np.argmax(val_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Additions (Exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T03:31:21.724723Z",
     "start_time": "2019-03-27T03:31:21.718539Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Other activation functions\n",
    "Implement activation functions such as:\n",
    "* Tanh\n",
    "* LeakyReLU\n",
    "* Sigmoid\n",
    "* ArcTan\n",
    "* Parameteric rectified linear unit (PReLU), [arxiv](https://arxiv.org/pdf/1502.01852.pdf). In which we have a Leaky ReLU with a learnable slope parameter.\n",
    "* Randomized leaky rectified linear unit (RReLU), [arxiv](https://arxiv.org/pdf/1505.00853.pdf)\n",
    "\n",
    "You can explore more in [here](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:21:42.701509Z",
     "start_time": "2019-03-31T23:21:42.691774Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        \"\"\"sigmoid layer simply applies elementwise sigmoid to all inputs\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # DO IT YOURSELF\n",
    "        return output\n",
    "    def backward(self, input, grad_output):\n",
    "        # DO IT YOURSELF\n",
    "        return grad_output*sigmoid_grad      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:21:45.622043Z",
     "start_time": "2019-03-31T23:21:45.612029Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LeakyReLU(Layer):\n",
    "    def __init__(self, slope = 0.01):\n",
    "        self.slope = slope\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = np.maximum(0,input) +self.slope * np.minimum(0,input)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        relu_grad = 1.*(input > 0)+ self.slope*(input < 0)\n",
    "        return grad_output*relu_grad        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:21:50.509407Z",
     "start_time": "2019-03-31T23:21:50.498195Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        \"\"\"Tanh layer simply applies elementwise sigmoid to all inputs\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = 2. / (1. + np.exp(-2.*input))-1.\n",
    "        return output\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        sigmoid_2x = 1. / (1. + np.exp(-2.*input))\n",
    "        tanh_grad = 4. * sigmoid_2x * (1. - sigmoid_2x)\n",
    "        return grad_output*tanh_grad      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T03:31:11.415079Z",
     "start_time": "2019-03-27T03:31:11.406256Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Initialization\n",
    "\n",
    "* Implement Dense layer with Xavier initialization as explained [here](http://bit.ly/2vTlmaJ)\n",
    "\n",
    "Check the influence of the initialization building a deeper network (6 or more layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### L1 (Lasso) and L2 (Ridge) regularization.\n",
    "\n",
    "* Implement a version of Dense layer with L2 regularization penalty: when updating Dense Layer weights, adjust gradients to minimize\n",
    "$$ \\mathcal{L}' = \\mathcal{L} + \\alpha \\cdot \\underset i \\sum {w_i}^2 \\, .$$\n",
    "\n",
    "\n",
    "* Implement a version of Dense layer with L1 regularization penalty: when updating Dense Layer weights, adjust gradients to minimize:\n",
    "$$ \\mathcal{L}' = \\mathcal{L} + \\alpha \\cdot \\underset i \\sum |w_i| \\, .$$\n",
    "Even if this loss function is non-differentiable at $w_i = 0$, there is no numerical problem, since it is differentiable up to a zero measure set. This regularization leads to sparse parameters, i.e., it sets many of them to zero, check that!\n",
    "\n",
    "\n",
    "* At the last exercise, combine both regularizations, with a different parameter for each of them.\n",
    "\n",
    "Conduct an experiment showing if regularization mitigates overfitting in case of abundantly large number of neurons. Consider tuning $\\alpha$ for better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Dropout\n",
    "\n",
    "Implement Dropout ([guide](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5)). \n",
    "\n",
    "Note, however, that those \"layers\" behave differently when training and when predicting on test set.\n",
    "\n",
    "* Dropout:\n",
    "  * During training: drop units randomly with probability __p__ and multiply everything by __1/(1-p)__\n",
    "  * During final predicton: do nothing; pretend there's no dropout\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Optimization\n",
    "Implement a version of Dense layer that uses momentum/rmsprop or whatever method you find in the literature.\n",
    "\n",
    "Most of those methods require persistent parameters like momentum direction or moving average grad norm, but you can easily store those parameters as attributes inside your layers.\n",
    "\n",
    "Compare this optimization methods with the basic SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Batch normalization ( Optimization and regularization)\n",
    "\n",
    "Implement Batch Normalization ([guide](https://gab41.lab41.org/batch-normalization-what-the-hey-d480039a9e3b)) .\n",
    "\n",
    "Note, however, that those \"layers\" behave differently when training and when predicting on test set.\n",
    "* Batch normalization\n",
    "  * During training, it substracts mean-over-batch and divides by std-over-batch and updates mean and variance.\n",
    "  * During final prediction, it uses accumulated mean and variance.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Testing different initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### LOADING NMIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:05:17.668951Z",
     "start_time": "2019-03-29T17:05:16.335431Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "loaded = np.load('NMIST_dataset.npz')\n",
    "X = loaded['X']\n",
    "X =  X.reshape((-1,X.shape[1]*X.shape[2]))\n",
    "# IMPORTANT: CHECK WHAT HAPPENS IF WE DON'T SCALE\n",
    "X = X/255.\n",
    "y = loaded['Y']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.3)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_val,y_val,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T17:05:18.380295Z",
     "start_time": "2019-03-29T17:05:17.671060Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Initialization test\n",
    "\n",
    "\n",
    "First, we define a Dense layer with an additional parameter called initializer, which allows us to select the initialization option.\n",
    "\n",
    "Bearing that in mind, we are going to define 3 identical 6 layer networks, but using differnet initializers:\n",
    "\n",
    "-network2: The constant variance that was defined by default.\n",
    "\n",
    "-network3: The \"XavierReLU\" with variance=2/(input_units)\n",
    "\n",
    "-network4: The \"XavierReLUBoth\" with variance=2*2/(input_units+output_units). This is the one originally proposed by Xavier and Bengio, but adding the 2 factor to take into account that we are using ReLU activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:44:36.365460Z",
     "start_time": "2019-03-31T23:44:36.333806Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"\n",
    "    Dense layer applying an affine transformation. \n",
    "    $$ X_{[l]}= X_{[l-1]} \\cdot W_{[l]} + b_{[l]} $$\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1,initializer=\"XavierReLU\", trainable = True):\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trainable = trainable\n",
    "        \n",
    "        # initialize weights with small random numbers. We use normal initialization, \n",
    "        # but surely there is something better. Try this once you got it working: http://bit.ly/2vTlmaJ\n",
    "        if initializer.lower() == \"XavierReLUBoth\".lower():\n",
    "            #For ReLU the variance has to be 2*2/(input_units+output_units)\n",
    "            self.initializer_scale = np.sqrt(4./(input_units+output_units))\n",
    "        elif initializer.lower() == \"XavierReLU\".lower():\n",
    "            #For ReLU the variance has to be 2/input_units\n",
    "            self.initializer_scale = np.sqrt(2./input_units)\n",
    "        elif initializer == \"XavierSoftmax\".lower():\n",
    "            #For Softmax we have an exponential with tangent at zero equal to 1. variance = 1/input_units\n",
    "            self.initializer_scale = np.sqrt(1./input_units)\n",
    "        else:\n",
    "            self.initializer_scale = 0.01\n",
    "            \n",
    "        self.weights = np.random.randn(input_units, output_units)*self.initializer_scale\n",
    "        self.biases = np.zeros(output_units)\n",
    "        \n",
    "        \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation.\n",
    "        input : np.array of shape=(items, input_units)\n",
    "        output : np.array of shape=(items, output_units)\n",
    "        \"\"\"\n",
    "        return np.matmul(input,self.weights)+self.biases\n",
    "    \n",
    "    def backward(self,input,grad_output):\n",
    "        \"\"\"\n",
    "        Backpropagate the gradient of the features and train the parameters\n",
    "        using Gradient Descent.\n",
    "        input : np.array of shape=(items, input_units)\n",
    "        grad_output : np.array of shape=(items, output_units)\n",
    "        \"\"\"\n",
    "        grad_input = np.matmul(grad_output,self.weights.T)\n",
    "        if self.trainable:\n",
    "        # compute gradient w.r.t. weights and biases\n",
    "            grad_weights = np.matmul(input.T,grad_output)\n",
    "            grad_biases = np.sum(grad_output,axis=0)\n",
    "            assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "            # Here we perform a stochastic gradient descent step. \n",
    "            # Later on, you can try replacing that with something better.\n",
    "            self.weights = self.weights - self.learning_rate * grad_weights\n",
    "            self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Deep network using the original initialization  var=(0.01)**2\n",
    "initi=\"normal\"\n",
    "network2 = []\n",
    "network2.append(Dense(X_train.shape[1],600,initializer=initi))\n",
    "network2.append(ReLU())\n",
    "network2.append(Dense(600,500,initializer=initi))\n",
    "network2.append(ReLU())\n",
    "network2.append(Dense(500,20,initializer=initi))\n",
    "network2.append(ReLU())\n",
    "network2.append(Dense(20,16,initializer=initi))\n",
    "network2.append(ReLU())\n",
    "network2.append(Dense(16,10,initializer=initi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Deep network using \"XavierReLU\"  var=2/(input_units)\n",
    "initi=\"XavierRELU\"\n",
    "network3 = []\n",
    "network3.append(Dense(X_train.shape[1],600,initializer=initi))\n",
    "network3.append(ReLU())\n",
    "network3.append(Dense(600,500,initializer=initi))\n",
    "network3.append(ReLU())\n",
    "network3.append(Dense(500,20,initializer=initi))\n",
    "network3.append(ReLU())\n",
    "network3.append(Dense(20,16,initializer=initi))\n",
    "network3.append(ReLU())\n",
    "network3.append(Dense(16,10,initializer=initi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Deep network using \"XavierReLUBoth\"  var=2*2/(input_units+output_units)\n",
    "initi=\"XavierReLUBoth\"\n",
    "network4 = []\n",
    "network4.append(Dense(X_train.shape[1],600,initializer=initi))\n",
    "network4.append(ReLU())\n",
    "network4.append(Dense(600,500,initializer=initi))\n",
    "network4.append(ReLU())\n",
    "network4.append(Dense(500,20,initializer=initi))\n",
    "network4.append(ReLU())\n",
    "network4.append(Dense(20,16,initializer=initi))\n",
    "network4.append(ReLU())\n",
    "network4.append(Dense(16,10,initializer=initi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_log2 = []\n",
    "val_log2 = []\n",
    "for epoch in range(20):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
    "        train(network2,x_batch,y_batch)\n",
    "    \n",
    "    train_log2.append(np.mean(predict(network2,X_train)==y_train))\n",
    "    val_log2.append(np.mean(predict(network2,X_val)==y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log2[-1])\n",
    "    print(\"Val accuracy:\",val_log2[-1])\n",
    "    plt.plot(train_log2,label='train accuracy')\n",
    "    plt.plot(val_log2,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "print('Maximum accuracy on validation set:',max(val_log), 'achieved at epoch:',np.argmax(val_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_log3 = []\n",
    "val_log3 = []\n",
    "for epoch in range(20):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
    "        train(network3,x_batch,y_batch)\n",
    "    \n",
    "    train_log3.append(np.mean(predict(network3,X_train)==y_train))\n",
    "    val_log3.append(np.mean(predict(network3,X_val)==y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log3[-1])\n",
    "    print(\"Val accuracy:\",val_log3[-1])\n",
    "    plt.plot(train_log3,label='train accuracy')\n",
    "    plt.plot(val_log3,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "print('Maximum accuracy on validation set:',max(val_log), 'achieved at epoch:',np.argmax(val_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_log4 = []\n",
    "val_log4 = []\n",
    "for epoch in range(20):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
    "        train(network4,x_batch,y_batch)\n",
    "    \n",
    "    train_log4.append(np.mean(predict(network4,X_train)==y_train))\n",
    "    val_log4.append(np.mean(predict(network4,X_val)==y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log4[-1])\n",
    "    print(\"Val accuracy:\",val_log4[-1])\n",
    "    plt.plot(train_log4,label='train accuracy')\n",
    "    plt.plot(val_log4,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "print('Maximum accuracy on validation set:',max(val_log), 'achieved at epoch:',np.argmax(val_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### ANALYSIS\n",
    "\n",
    "We can observe that using the predefined initializer suffers from vanishing gradient problems, since the results are static through all the training process. This is due to the small variance of the weights when being initialized, leading to them being too small and the backpropagation algorithm to be not efficient.\n",
    "\n",
    "So, in that case network4(\"XavierReLUBoth\") performs slightly better, and both Xavier initializers much better than the constant one. Even having one layer less, it suffers from vanishing gradient problem. \n",
    "\n",
    "We can conclude that weight initialization can have a great impact on the learning stage, and that Xavier initialization is a really nice option to consider when deciding how to initialize our parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODER TEST\n",
    "\n",
    "In this part, we are gonna build a network with a bottleneck, this is, the last layers will impose that we have to encode each item in a 2 dimensional state, i.e., using only two features. As you will see, the overfitting will be reduced in this setting, because the bottleneck is regularizing.\n",
    "\n",
    "This forces the network to have a representation of the images in this space, which is linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:36:14.227145Z",
     "start_time": "2019-03-31T23:36:14.187959Z"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "from ipywidgets import interact\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us show how the different classes are distributed in the original feature space (using PCA to plot it in a 2D figure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:56:32.214387Z",
     "start_time": "2019-03-31T23:56:30.004231Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "X_test_PCA = pca.fit_transform(X_test)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for i in range(10):\n",
    "    plt.scatter(X_test_PCA[y_test==i,0],X_test_PCA[y_test==i,1],label = str(i),alpha=0.3)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:36:15.466698Z",
     "start_time": "2019-03-31T23:36:15.369995Z"
    }
   },
   "outputs": [],
   "source": [
    "network4 = pickle.load(open('network4.pickle', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T04:02:54.739479Z",
     "start_time": "2019-03-27T03:56:35.910238Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Deep network using \"XavierReLUBoth\"  var=2*2/(input_units+output_units)\n",
    "# initi=\"XavierReLUBoth\"\n",
    "# network4 = []\n",
    "# network4.append(Dense(X_train.shape[1],512,initializer=initi))\n",
    "# network4.append(ReLU())\n",
    "# network4.append(Dense(512,256,initializer=initi))\n",
    "# network4.append(ReLU())\n",
    "# network4.append(Dense(256,32,initializer=initi))\n",
    "# network4.append(ReLU())\n",
    "# network4.append(Dense(32,2,initializer=initi))\n",
    "# network4.append(ReLU())\n",
    "# network4.append(Dense(2,2,initializer=initi))\n",
    "# network4.append(Tanh())\n",
    "# network4.append(Dense(2,10,initializer=initi))\n",
    "\n",
    "\n",
    "# train_log4 = []\n",
    "# val_log4 = []\n",
    "# for epoch in range(20):\n",
    "\n",
    "#     for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
    "#         train(network4,x_batch,y_batch)\n",
    "    \n",
    "#     train_log4.append(np.mean(predict(network4,X_train)==y_train))\n",
    "#     val_log4.append(np.mean(predict(network4,X_val)==y_val))\n",
    "    \n",
    "#     clear_output()\n",
    "#     print(\"Epoch\",epoch)\n",
    "#     print(\"Train accuracy:\",train_log4[-1])\n",
    "#     print(\"Val accuracy:\",val_log4[-1])\n",
    "#     plt.plot(train_log4,label='train accuracy')\n",
    "#     plt.plot(val_log4,label='val accuracy')\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "# print('Maximum accuracy on validation set:',max(val_log4), 'achieved at epoch:',np.argmax(val_log4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('network4.pickle', \"wb\") as output_file:\n",
    "#     pickle.dump(network4, output_file,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:36:22.579726Z",
     "start_time": "2019-03-31T23:36:21.718920Z"
    }
   },
   "outputs": [],
   "source": [
    "(predict(network4,X_val)==y_val).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:40:32.016161Z",
     "start_time": "2019-03-31T23:40:29.632550Z"
    }
   },
   "outputs": [],
   "source": [
    "enc_list = [X_test_PCA]\n",
    "for i in reversed(range(1,5)):\n",
    "    enc_list.append(forward(network4,X_test)[-i])\n",
    "\n",
    "@interact\n",
    "def show_articles_more_than(step=(0, len(enc_list)-1, 1)):\n",
    "    enc = enc_list[step]\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    for i in range(10):\n",
    "        plt.scatter(enc[y_test==i,0],enc[y_test==i,1],label = str(i),alpha=0.3)\n",
    "    plt.legend()\n",
    "    return None\n",
    "return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:41:18.315020Z",
     "start_time": "2019-03-31T23:41:18.275095Z"
    }
   },
   "outputs": [],
   "source": [
    "network5 = pickle.load(open('network5.pickle', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T04:15:29.828889Z",
     "start_time": "2019-03-27T04:08:35.645104Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Deep network using \"XavierReLUBoth\"  var=2*2/(input_units+output_units)\n",
    "# initi=\"XavierReLUBoth\"\n",
    "# network5 = []\n",
    "# network5.append(Dense(X_train.shape[1],512,initializer=initi))\n",
    "# network5.append(Tanh())\n",
    "# network5.append(Dense(512,256,initializer=initi))\n",
    "# network5.append(Tanh())\n",
    "# network5.append(Dense(256,32,initializer=initi))\n",
    "# network5.append(Tanh())\n",
    "# network5.append(Dense(32,2,initializer=initi))\n",
    "# network5.append(Tanh())\n",
    "# network5.append(Dense(2,2,initializer=initi))\n",
    "# network5.append(Tanh())\n",
    "# network5.append(Dense(2,10,initializer=initi))\n",
    "\n",
    "\n",
    "# train_log5 = []\n",
    "# val_log5 = []\n",
    "# for epoch in range(20):\n",
    "\n",
    "#     for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
    "#         train(network5,x_batch,y_batch)\n",
    "    \n",
    "#     train_log5.append(np.mean(predict(network5,X_train)==y_train))\n",
    "#     val_log5.append(np.mean(predict(network5,X_val)==y_val))\n",
    "    \n",
    "#     clear_output()\n",
    "#     print(\"Epoch\",epoch)\n",
    "#     print(\"Train accuracy:\",train_log5[-1])\n",
    "#     print(\"Val accuracy:\",val_log5[-1])\n",
    "#     plt.plot(train_log5,label='train accuracy')\n",
    "#     plt.plot(val_log5,label='val accuracy')\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "# print('Maximum accuracy on validation set:',max(train_log5), 'achieved at epoch:',np.argmax(train_log5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T04:15:35.289428Z",
     "start_time": "2019-03-27T04:15:35.269866Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('network5.pickle', \"wb\") as output_file:\n",
    "#     pickle.dump(network5, output_file,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:41:30.284079Z",
     "start_time": "2019-03-31T23:41:27.439096Z"
    }
   },
   "outputs": [],
   "source": [
    "enc_list = [X_test_PCA]\n",
    "for i in reversed(range(1,5)):\n",
    "    enc_list.append(forward(network5,X_test)[-i])\n",
    "\n",
    "@interact\n",
    "def show_articles_more_than(step=(0, len(enc_list)-1, 1)):\n",
    "    enc = enc_list[step]\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    for i in range(10):\n",
    "        plt.scatter(enc[y_test==i,0],enc[y_test==i,1],label = str(i),alpha=0.3)\n",
    "    plt.legend()\n",
    "    return None\n",
    "return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:59:16.264729Z",
     "start_time": "2019-03-31T23:59:16.222894Z"
    }
   },
   "outputs": [],
   "source": [
    "network6 = pickle.load(open('network6.pickle', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:56:29.993618Z",
     "start_time": "2019-03-31T23:47:51.540358Z"
    }
   },
   "outputs": [],
   "source": [
    "# initi=\"XavierReLUBoth\"\n",
    "# learning_rate = 0.02\n",
    "# network6 = []\n",
    "# network6.append(Dense(X_train.shape[1],512,initializer=initi,learning_rate=learning_rate))\n",
    "# network6.append(LeakyReLU())\n",
    "# network6.append(Dense(512,256,initializer=initi,learning_rate=learning_rate))\n",
    "# network6.append(LeakyReLU())\n",
    "# network6.append(Dense(256,32,initializer=initi,learning_rate=learning_rate))\n",
    "# network6.append(LeakyReLU())\n",
    "# network6.append(Dense(32,2,initializer=initi,learning_rate=learning_rate))\n",
    "# network6.append(LeakyReLU())\n",
    "# network6.append(Dense(2,2,initializer=initi,learning_rate=learning_rate))\n",
    "# network6.append(LeakyReLU())\n",
    "# network6.append(Dense(2,10,initializer=initi,learning_rate=learning_rate))\n",
    "\n",
    "\n",
    "# train_log5 = []\n",
    "# val_log5 = []\n",
    "# for epoch in range(20):\n",
    "\n",
    "#     for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
    "#         train(network6,x_batch,y_batch)\n",
    "    \n",
    "#     train_log5.append(np.mean(predict(network6,X_train)==y_train))\n",
    "#     val_log5.append(np.mean(predict(network6,X_val)==y_val))\n",
    "    \n",
    "#     clear_output()\n",
    "#     print(\"Epoch\",epoch)\n",
    "#     print(\"Train accuracy:\",train_log5[-1])\n",
    "#     print(\"Val accuracy:\",val_log5[-1])\n",
    "#     plt.plot(train_log5,label='train accuracy')\n",
    "#     plt.plot(val_log5,label='val accuracy')\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "# print('Maximum accuracy on validation set:',max(val_log5), 'achieved at epoch:',np.argmax(val_log5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:58:52.744831Z",
     "start_time": "2019-03-31T23:58:52.723991Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('network6.pickle', \"wb\") as output_file:\n",
    "#     pickle.dump(network6, output_file,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T23:59:25.683608Z",
     "start_time": "2019-03-31T23:59:23.084201Z"
    }
   },
   "outputs": [],
   "source": [
    "enc_list = [X_test_PCA]\n",
    "for i in reversed(range(1,5)):\n",
    "    enc_list.append(forward(network6,X_test)[-i])\n",
    "\n",
    "@interact\n",
    "def show_articles_more_than(step=(0, len(enc_list)-1, 1)):\n",
    "    enc = enc_list[step]\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    for i in range(10):\n",
    "        plt.scatter(enc[y_test==i,0],enc[y_test==i,1],label = str(i),alpha=0.3)\n",
    "    plt.legend()\n",
    "    return None\n",
    "return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: The old binary classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the methods shown above to solve the classification problem of the previous lecture. How the metrics compare with the methods we used before?\n",
    "\n",
    "Try to use a NN in which all the layers have 2 neurons, i.e., we are always dealing with a 2-dimensional feature space. Check how the NN is transforming the points after applying each layer.\n",
    "\n",
    "You can also play with larger layers (more neurons) and compare the metrics. If you want a way of plotting the features of that layers, you can use a PCA projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T00:19:17.021459Z",
     "start_time": "2019-04-01T00:19:16.687784Z"
    }
   },
   "outputs": [],
   "source": [
    "dic_data_A = {'n': 200 ,'mean': (0,2), 'cov' :((1,0),(0,2)), 'y' : 0 }  # RED\n",
    "dic_data_B = {'n': 250 ,'mean': (-1,-2), 'cov' :((3,0),(0,1)), 'y' : 1 }  # BLUE\n",
    "dic_data_B2 = {'n': 250 ,'mean': (-3,3), 'cov' :((1,0),(0,2)), 'y' : 1 }  # BLUE\n",
    "# dic_data = {'A': dic_data_A, 'B' : dic_data_B }\n",
    "dic_data = {'A': dic_data_A, 'B' : dic_data_B , 'B2' : dic_data_B2 }\n",
    "\n",
    "# We sample the points with numpy.random\n",
    "np.random.seed(1)\n",
    "samples = {key : np.random.multivariate_normal(dic['mean'], np.array(dic['cov']), dic['n']) \n",
    "           for key,dic in dic_data.items()}\n",
    "     \n",
    "X = np.concatenate(tuple(samples[key] for key in dic_data.keys() ),axis=0)\n",
    "Y = np.concatenate(tuple(dic['y']* np.ones(dic['n'], dtype='int') \n",
    "                         for key,dic in dic_data.items() ), axis=0)\n",
    "\n",
    "# Train Val Test Split. 60% / 20% / 20%.\n",
    "X_train,X_val,y_train, y_val = train_test_split(X,Y,test_size = 0.4)\n",
    "X_val,X_test,y_val, y_test = train_test_split(X_val,y_val,test_size = 0.5)\n",
    "\n",
    "# We transform into coordinates in the plane to make plotting easier.\n",
    "colors = [\n",
    "    \"#%02x%02x%02x\" % (int(r), int(g), int(b)) for r, g ,b  in 150*np.eye(3)[[0,2,1]][np.array(y_train,dtype='int')]\n",
    "]\n",
    "colors_val = [\n",
    "    \"#%02x%02x%02x\" % (int(r), int(g), int(b)) for r, g ,b  in 150*np.eye(3)[[0,2,1]][np.array(y_val,dtype='int')]\n",
    "]\n",
    "colors_test = [\n",
    "    \"#%02x%02x%02x\" % (int(r), int(g), int(b)) for r, g ,b  in 150*np.eye(3)[[0,2,1]][np.array(y_test,dtype='int')]\n",
    "]\n",
    "x0_range = (X[:,0].min()-1,X[:,0].max()+1)\n",
    "x1_range = (X[:,1].min()-1,X[:,1].max()+1)\n",
    "x0 = X_train[:,0] \n",
    "x1 = X_train[:,1] \n",
    "x0_val = X_val[:,0] \n",
    "x1_val = X_val[:,1] \n",
    "x0_test = X_test[:,0] \n",
    "x1_test = X_test[:,1] \n",
    "\n",
    "import seaborn as sns\n",
    "sns.scatterplot(x0,x1,hue=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T00:21:13.700123Z",
     "start_time": "2019-04-01T00:20:57.932497Z"
    }
   },
   "outputs": [],
   "source": [
    "initi=\"XavierReLUBoth\"\n",
    "learning_rate = 0.01\n",
    "network = []\n",
    "network.append(Dense(X_train.shape[1],16,initializer=initi,learning_rate=learning_rate))\n",
    "network.append(LeakyReLU())\n",
    "network.append(Dense(16,8,initializer=initi,learning_rate=learning_rate))\n",
    "network.append(LeakyReLU())\n",
    "network.append(Dense(8,4,initializer=initi,learning_rate=learning_rate))\n",
    "network.append(LeakyReLU())\n",
    "network.append(Dense(4,2,initializer=initi,learning_rate=learning_rate))\n",
    "network.append(LeakyReLU())\n",
    "network.append(Dense(2,2,initializer=initi,learning_rate=learning_rate))\n",
    "network.append(LeakyReLU())\n",
    "network.append(Dense(2,2,initializer=initi,learning_rate=learning_rate))\n",
    "\n",
    "\n",
    "train_log = []\n",
    "val_log = []\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
    "        train(network,x_batch,y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(predict(network,X_train)==y_train))\n",
    "    val_log.append(np.mean(predict(network,X_val)==y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log[-1])\n",
    "    print(\"Val accuracy:\",val_log[-1])\n",
    "    plt.plot(train_log,label='train accuracy')\n",
    "    plt.plot(val_log,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "print('Maximum accuracy on validation set:',max(val_log), 'achieved at epoch:',np.argmax(val_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T00:21:22.085209Z",
     "start_time": "2019-04-01T00:21:21.672499Z"
    }
   },
   "outputs": [],
   "source": [
    "# pca = PCA(n_components = 2)\n",
    "# X_val_PCA = pca.fit_transform(X_val)\n",
    "enc_list = [X_val]\n",
    "for i in reversed(range(1,5)):\n",
    "    enc_list.append(forward(network,X_val)[-i])\n",
    "\n",
    "@interact\n",
    "def show_articles_more_than(step=(0, len(enc_list)-1, 1)):\n",
    "    enc = enc_list[step]\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    for i in np.unique(y_val):\n",
    "        plt.scatter(enc[y_val==i,0],enc[y_val==i,1],label = str(i),alpha=0.3)\n",
    "    plt.legend()\n",
    "    return None\n",
    "return None"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.2",
    "jupytext_version": "1.0.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "264px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
