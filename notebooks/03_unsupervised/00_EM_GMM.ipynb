{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation-maximization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will implement the Expectation-Maximization algorithm for a Gaussian Mixture Model, commonly used for perform soft (with probabilities) clustering of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "* For a description of the Expectation-Maximization (EM) algorithm for a Gaussian Mixture Model:\n",
    "\n",
    "    \"Pattern Recognition and Machine Learning\" by C. Bishop, 9.2 p. 430\n",
    "\n",
    "\n",
    "* The algorithm:\n",
    "\n",
    "    \"Pattern Recognition and Machine Learning\" by C. Bishop, 9.2.2 p. 438\n",
    "\n",
    "\n",
    "* For a demonstration of the algorithm and its convergence (to a local maximum of the likelihood):\n",
    "\n",
    "    \"Pattern Recognition and Machine Learning\" by C. Bishop, 9.4 p. 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:31:33.848296Z",
     "start_time": "2019-04-07T23:31:28.598811Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "import seaborn as sns\n",
    "import random\n",
    "sns.set()\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "np.random.seed(1)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing EM for GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For debugging we will use samples from gaussian mixture model with unknown mean, variance and priors. We also added inital values of parameters for grading purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:31:33.872592Z",
     "start_time": "2019-04-07T23:31:33.859283Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded = np.load('data.npz')\n",
    "X = loaded['X']\n",
    "pi_test=loaded['pi_test']\n",
    "mu_test=loaded['mu_test']\n",
    "sigma_test=loaded['sigma_test']\n",
    "\n",
    "# We transform into coordinates in the plane to make plotting easier.\n",
    "x0_range = (X[:,0].min()-1,X[:,0].max()+1)\n",
    "x1_range = (X[:,1].min()-1,X[:,1].max()+1)\n",
    "x0 = X[:,0] \n",
    "x1 = X[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:44:59.289542Z",
     "start_time": "2019-04-07T23:44:58.965504Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.set_xlim(x0_range)\n",
    "ax.set_ylim(x1_range)\n",
    "sns.scatterplot(x = x0, y = x1, color='Black', s=40,alpha = 0.8,legend='full', ax=ax)\n",
    "ax.figure.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent variables $Z$ are indices of components to which each data point is assigned. $Z_n$ (class index for item $n$) is a one-hot encoded vector. For example, if we have $C=3$ classes and object $n$ lies in first class, $Z_n = [1, 0, 0]$.\n",
    "\n",
    "The joint distribution can be written as follows: \n",
    "\\begin{align}\n",
    " p(Z, X \\mid \\theta) =  p(X \\mid Z,\\theta) p(Z \\mid \\theta) =  \\\\\n",
    " \\prod\\limits_{n=1}^N p(Z_n, X_n \\mid \\theta) =\n",
    "\\prod\\limits_{i=n}^N \\prod\\limits_{c=1}^C [\\pi_c \\mathcal{N}(X_n \\mid \\mu_c, \\Sigma_c)]^{Z_{nc}} \\;.\n",
    "\\end{align}\n",
    "where the multivariate normal distribution is given by\n",
    "$$\n",
    "\\mathcal{N}(x \\mid \\mu_c, \\Sigma_c) = \\frac{1}{\\sqrt{(2\\pi)^d | \\Sigma_c |}} \\exp{\\left[ \n",
    "-\\frac{1}{2}\n",
    "(x-\\mu_c)\\Sigma_c^{-1}(x-\\mu_c)\n",
    "\\right]}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "We first initialize the parameters of the GMM, $\\theta$, which consists of mean vectors $\\mu_c$, covariance matrices $\\Sigma_c$ and priors $\\pi_c$ for each component.\n",
    "\n",
    "#### E-step\n",
    "\n",
    "Considering the parameters as fixed, we calculate the responsabilities $\\gamma_{nc}$. The probability of the point $X_n$ to belong to class $c$. \n",
    "$$\n",
    "\\gamma_{nc} \\equiv P(Z_{nc} = 1 \\mid X_n, \\theta)\\,,\n",
    "$$\n",
    "where $\\sum\\limits_{c=1}^C\\gamma_{nc}=1$.\n",
    "\n",
    "It is obtained with\n",
    "$$\n",
    "\\gamma_{nc} = \\frac{  \\pi_c \\mathcal{N}(X_n \\mid \\mu_c, \\Sigma_c)}{ \\sum\\limits_{c'=1}^C \\pi_{c'} \\mathcal{N}(X_n \\mid \\mu_{c'}, \\Sigma_{c'})}\n",
    "$$\n",
    "\n",
    "#### M-step\n",
    "\n",
    "We reestimate the parameters considering the responsabilities as fixed. This is really intuitive, because we have to use the Maximum Likelihood Estimate formulae for the mean and covariance matrix of a Gaussian, weightening with the responsabilities:\n",
    "\n",
    "$$\n",
    "\\mu_{c,i} = \\frac{1}{N_c}{\\sum_n}\\gamma_{nc} x_i^{(n)}\n",
    "\\,,\n",
    "$$\n",
    "$$\n",
    "\\Sigma_{c,ij} = \\frac{1}{N_c}{\\sum_n}\\gamma_{nc}  \n",
    "\\left(x^{(n)}-\\mu_c \\right)_i\n",
    "\\left(x^{(n)}-\\mu_c \\right)_j\n",
    "\\,,\n",
    "$$\n",
    "$$\n",
    "\\pi_c = \\frac{N_c}{N}\n",
    "\\,,\n",
    "$$\n",
    "where $N_c \\equiv \\sum_n \\gamma_{nc}$\n",
    "\n",
    "#### Check convergence\n",
    "\n",
    "Evaluate the log likelihood and check the convergence of either the parameters or the log likelihood.\n",
    "\n",
    "$$\\mathcal{L} = \\log{ p( X \\mid \\theta)  } = \n",
    "\\sum_n \\log{ \\left[ \\sum_{c=1}^{C} \\pi_c \\mathcal{N}(X_n \\mid \\mu_c, \\Sigma_c)   \\right] } \\;.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Trick 1:</b> It is important to avoid numerical errors. At some point you will have to compute the formula of the following form: $\\frac{e^{x_i}}{\\sum_j e^{x_j}}$. When you compute exponents of large numbers, you get huge numerical errors (some numbers will simply become infinity). You can avoid this by dividing numerator and denominator by $e^{\\max(x)}$: $\\frac{e^{x_i-\\max(x)}}{\\sum_j e^{x_j - \\max(x)}}$. After this transformation maximum value in the denominator will be equal to one. All other terms will contribute smaller values. This trick is called log-sum-exp. So, to compute desired formula you first subtract maximum value from each component in vector $X$ and then compute everything else as before.\n",
    "\n",
    "<b>Trick 2:</b> You will probably need to compute formula of the form $A^{-1}x$ at some point. You would normally inverse $A$ and then multiply it by $x$. A bit faster and more numerically accurate way to do this is to solve the equation $Ay = x$. Its solution is $y=A^{-1}x$, but the equation $Ay = x$ can be solved by Gaussian elimination procedure. You can use ```np.linalg.solve``` for this.\n",
    "\n",
    "<b>Other usefull functions: </b> <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.slogdet.html\">```slogdet```</a> and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.det.html#numpy.linalg.det\">```det```</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:32:28.793620Z",
     "start_time": "2019-04-07T23:32:28.786728Z"
    }
   },
   "outputs": [],
   "source": [
    "def E_step(X, pi, mu, sigma):\n",
    "    \"\"\"\n",
    "    Performs E-step on GMM model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.array, shape=(N x d)\n",
    "        Features matrix.\n",
    "    pi: np.array, shape=(C)\n",
    "        mixture component weights.\n",
    "    mu: np.array, shape=(C x d) \n",
    "        mixture component means.\n",
    "    sigma: np.array, shape=(C x d x d) \n",
    "        mixture component covariance matrices.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gamma: np.array, shape=(N x C)\n",
    "        Updated responsabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = pi.shape[0] # number of clusters\n",
    "    d = mu.shape[1] # dimension of each object\n",
    "    gamma = np.zeros((N, C)) # Initialize responsabilities\n",
    "                \n",
    "    ### Calculate the new responsabilities\n",
    "\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:32:38.950270Z",
     "start_time": "2019-04-07T23:32:38.901442Z"
    }
   },
   "outputs": [],
   "source": [
    "gamma = E_step(X, pi_test, mu_test, sigma_test)\n",
    "gamma[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:32:39.814532Z",
     "start_time": "2019-04-07T23:32:39.806377Z"
    }
   },
   "outputs": [],
   "source": [
    "def M_step(X, gamma):\n",
    "    \"\"\"\n",
    "    Performs M-step on GMM model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.array, shape=(N x d)\n",
    "        Features matrix.\n",
    "    gamma: np.array, shape=(N x C)\n",
    "        Updated responsabilities.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pi: np.array, shape=(C)\n",
    "        Updated mixture component weights .\n",
    "    mu: np.array, shape=(C x d) \n",
    "        Updated mixture component means.\n",
    "    sigma: np.array, shape=(C x d x d) \n",
    "        Updated mixture component covariance matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = gamma.shape[1] # number of clusters\n",
    "    d = X.shape[1] # dimension of each object\n",
    "    sigma=np.zeros((C,d,d))\n",
    "\n",
    "    ### Calculate the updated pi,mu and sigma\n",
    "    \n",
    "  \n",
    "\n",
    "    return pi, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:32:40.488969Z",
     "start_time": "2019-04-07T23:32:40.425429Z"
    }
   },
   "outputs": [],
   "source": [
    "gamma = E_step(X, pi_test, mu_test, sigma_test)\n",
    "pi, mu, sigma = M_step(X, gamma)\n",
    "print(pi,mu,sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need some function to track convergence. We will stop our EM iterations when $\\mathcal{L}$ will saturate. Usually, you will need only about 10-20 iterations to converge. It is also useful to check that this function never decreases during training. If it does, you have a bug in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:32:42.597927Z",
     "start_time": "2019-04-07T23:32:42.587058Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss(X, pi, mu, sigma, gamma):\n",
    "    \"\"\"\n",
    "    Compute loss function on GMM model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.array, shape=(N x d)\n",
    "        Features matrix.\n",
    "    pi: np.array, shape=(C)\n",
    "        Updated mixture component weights .\n",
    "    mu: np.array, shape=(C x d) \n",
    "        Updated mixture component means.\n",
    "    sigma: np.array, shape=(C x d x d) \n",
    "        Updated mixture component covariance matrices.\n",
    "    gamma: np.array, shape=(N x C)\n",
    "        Updated responsabilities.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        Log likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = gamma.shape[1] # number of clusters\n",
    "    d = X.shape[1] # dimension of each object\n",
    "    \n",
    "    ### Calculate the log likelihood. \n",
    "    # Be careful with the numerical errors.\n",
    "    \n",
    "\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:32:44.723692Z",
     "start_time": "2019-04-07T23:32:44.623703Z"
    }
   },
   "outputs": [],
   "source": [
    "pi, mu, sigma = pi_test, mu_test, sigma_test\n",
    "gamma = E_step(X, pi, mu, sigma)\n",
    "pi, mu, sigma = M_step(X, gamma)\n",
    "loss = compute_loss(X, pi, mu, sigma, gamma)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to combine everything into a training loop. \n",
    "\n",
    "We will start at random values of $\\pi$, $\\mu$ and $\\Sigma$, train until $\\mathcal{L}$ stops changing and return the resulting points. We also know that EM algorithm sometimes stops at local optima. To avoid this we should restart algorithm multiple times from different starting positions. Each training trial should stop either when  a maximum number of iterations is reached or when relative improvement is smaller than given tolerance ($|\\frac{\\mathcal{L}_i-\\mathcal{L}_{i-1}}{\\mathcal{L}_{i-1}}| \\le \\text{rtol}$).\n",
    "\n",
    "Remember, that values of $\\pi$ that you generate must be non-negative and sum up to 1. Also, $\\Sigma$ matrices must be symmetric and positive semi-definite. If you don't know how to generate those matrices, you can use $\\Sigma=I$ as initialization.\n",
    "\n",
    "You will also sometimes get numerical errors because of component collapsing. The easiest way to deal with this problems is to simply restart the procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:32:47.863727Z",
     "start_time": "2019-04-07T23:32:47.848872Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_EM(X, C, rtol=0.5e-3, max_iter=50, restarts=15, verbose = False):\n",
    "    \"\"\"\n",
    "    Starts with random initialization *restarts* times\n",
    "    Runs optimization until saturation with *rtol* reached\n",
    "    or *max_iter* iterations were made.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.array, shape=(N x d)\n",
    "        Features matrix.\n",
    "    C: int \n",
    "        Number of classes.\n",
    "    rtol: float, default=0.5e-3\n",
    "        Maximum relative tolerance to stop the training loop.\n",
    "    max_iter: int, default=50\n",
    "        Maximum number of iterations in the training loop.\n",
    "    restarts: int, default=15\n",
    "        Number of initializations of the parameters.\n",
    "    verbose: bool, default = False\n",
    "        If True, print errors and the loss at each restart.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    best_loss: float\n",
    "        Log likelihood of the best fit.\n",
    "    best_pi: np.array, shape=(C)\n",
    "        Best fit of mixture component weights \n",
    "    best_mu: np.array, shape=(C x d) \n",
    "        Best fit of mixture component means\n",
    "    best_sigma: np.array, shape=(C x d x d) \n",
    "        Best fit of mixture component covariance matrices\n",
    "    \"\"\"\n",
    "\n",
    "    N = X.shape[0] # number of objects\n",
    "    d = X.shape[1] # dimension of each object\n",
    "    best_loss = -10**(100)\n",
    "    best_pi = None\n",
    "    best_mu = None\n",
    "    best_sigma = None\n",
    "    \n",
    "    for restart_index in range(restarts):\n",
    "        try:\n",
    "            ### Initialize the parameters: pi, mu and sigma\n",
    "\n",
    "            loss0 = -10**(100)\n",
    "            \n",
    "            for i in range(max_iter):\n",
    "                ### Apply the E and M steps and compute the loss.\n",
    "            \n",
    "                gamma = E_step(X, pi, mu, sigma)\n",
    "                pi, mu, sigma = M_step(X, gamma)\n",
    "                loss = compute_loss(X, pi, mu, sigma, gamma)\n",
    "                # print(\"Iteration=\",i,\" Loss=\",loss)\n",
    "                \n",
    "                ### Check the convergence properties\n",
    "                if loss < loss0:\n",
    "                    loss=-10**(100)\n",
    "                    if verbose: print(restart_index,\") CONVERGENCE PROBLEM\")\n",
    "                    break\n",
    "                if ( np.abs(1.-loss/loss0) <= rtol ):\n",
    "                    if verbose: print(restart_index,\") Loss has converged=\",loss)       \n",
    "                    break\n",
    "                loss0=loss\n",
    "                \n",
    "            # We save the best parameters and loss\n",
    "            if loss > best_loss:\n",
    "                best_loss, best_pi, best_mu, best_sigma = loss,pi,mu,sigma\n",
    "        except np.linalg.LinAlgError:\n",
    "            if verbose: print(restart_index,\") Singular matrix: components collapsed\")\n",
    "            pass\n",
    "\n",
    "    return best_loss, best_pi, best_mu, best_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the algorithm is correct, it should converge in about 20 iterations. In order to compute the probabilities, we use the `E_step` function to compute the responsabilities of the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:32:58.187138Z",
     "start_time": "2019-04-07T23:32:50.260756Z"
    }
   },
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "best_loss, best_pi, best_mu, best_sigma = train_EM(X, n_classes, verbose=True, restarts=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Testing the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:43:33.336453Z",
     "start_time": "2019-04-07T23:43:13.592728Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "best_loss, best_pi, best_mu, best_sigma = train_EM(X, n_classes)\n",
    "\n",
    "gamma = E_step(X, best_pi, best_mu, best_sigma)\n",
    "labels = gamma.argmax(1)\n",
    "print('Number of classes:', n_classes)\n",
    "print('Best loss:',best_loss)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.set_xlim(x0_range)\n",
    "ax.set_ylim(x1_range)\n",
    "sns.scatterplot(x = x0, y = x1, hue=labels, s=40, alpha = 0.8,legend='full', ax=ax)\n",
    "ax.figure.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:42:50.276177Z",
     "start_time": "2019-04-07T23:42:15.517504Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_classes = 4\n",
    "best_loss, best_pi, best_mu, best_sigma = train_EM(X, n_classes)\n",
    "\n",
    "gamma = E_step(X, best_pi, best_mu, best_sigma)\n",
    "labels = gamma.argmax(1)\n",
    "print('Number of classes:', n_classes)\n",
    "print('Best loss:',best_loss)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.set_xlim(x0_range)\n",
    "ax.set_ylim(x1_range)\n",
    "sns.scatterplot(x = x0, y = x1, hue=labels, s=40, alpha = 0.8,legend='full', ax=ax)\n",
    "ax.figure.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:44:18.696442Z",
     "start_time": "2019-04-07T23:43:33.340123Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_classes = 5\n",
    "best_loss, best_pi, best_mu, best_sigma = train_EM(X, n_classes)\n",
    "\n",
    "gamma = E_step(X, best_pi, best_mu, best_sigma)\n",
    "labels = gamma.argmax(1)\n",
    "print('Number of classes:', n_classes)\n",
    "print('Best loss:',best_loss)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.set_xlim(x0_range)\n",
    "ax.set_ylim(x1_range)\n",
    "sns.scatterplot(x = x0, y = x1, hue=labels, s=40, alpha = 0.8,legend='full', ax=ax)\n",
    "ax.figure.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:44:58.961564Z",
     "start_time": "2019-04-07T23:44:18.699943Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_classes = 6\n",
    "best_loss, best_pi, best_mu, best_sigma = train_EM(X, n_classes)\n",
    "\n",
    "gamma = E_step(X, best_pi, best_mu, best_sigma)\n",
    "labels = gamma.argmax(1)\n",
    "print('Number of classes:', n_classes)\n",
    "print('Best loss:',best_loss)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.set_xlim(x0_range)\n",
    "ax.set_ylim(x1_range)\n",
    "sns.scatterplot(x = x0, y = x1, hue=labels, s=40, alpha = 0.8,legend='full', ax=ax)\n",
    "ax.figure.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:35:59.807294Z",
     "start_time": "2019-04-07T23:35:59.796965Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded = np.load('data.npz')\n",
    "X = loaded['X']\n",
    "pi_test=loaded['pi_test']\n",
    "mu_test=loaded['mu_test']\n",
    "sigma_test=loaded['sigma_test']\n",
    "Y = np.load('labels.npz')['Y']\n",
    "\n",
    "# # We transform into coordinates in the plane to make plotting easier.\n",
    "\n",
    "x0_range = (X[:,0].min()-1,X[:,0].max()+1)\n",
    "x1_range = (X[:,1].min()-1,X[:,1].max()+1)\n",
    "x0 = X[:,0] \n",
    "x1 = X[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T23:44:59.841232Z",
     "start_time": "2019-04-07T23:44:59.292805Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111,aspect='equal')\n",
    "ax.set_xlim(x0_range)\n",
    "ax.set_ylim(x1_range)\n",
    "sns.scatterplot(x = x0, y = x1, hue = Y, alpha = 0.8,s=40, legend='full', ax=ax)\n",
    "ax.figure.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.2",
    "jupytext_version": "1.0.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
