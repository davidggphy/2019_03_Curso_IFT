{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T19:54:03.136307Z",
     "start_time": "2019-04-07T19:54:03.113155Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style()\n",
    "import pandas as pd\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:**\n",
    "\n",
    "\"Pattern Recognition and Machine Learning\" by C. Bishop, 12.1 p. 561"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we are going to implement the Principal Component Analysis (PCA) algorithm from scratch using numpy and its [linear algebra routines](https://docs.scipy.org/doc/numpy/reference/routines.linalg.html) implemented in `numpy.linalg`.\n",
    "\n",
    "It can be defined as:\n",
    "* PCA is a **linear** dimension-reduction tool that can be used to reduce a large set of features to a small set that still contains most of the *information* in the large set. \n",
    "\n",
    "Or, a bit more precisely:\n",
    "* PCA is a mathematical procedure that **linearly** transforms a number of (possibly) correlated features into a (smaller) number of uncorrelated features called principal components.\n",
    "\n",
    "It **does NOT require labeled data**, i.e., it is an **unsupervised** algorithm. Traditionally, principal component analysis is performed on a square symmetric matrix. It can be the **Covariance matrix** (scaled sums of squares and cross products), or **Correlation matrix** (sums of squares and cross products from standardized data). The correlation matrix is used if the variances of different features differ much, or if the units of measurement of the features differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T18:43:51.451849Z",
     "start_time": "2019-04-07T18:43:51.435134Z"
    }
   },
   "source": [
    "In order to decrease the dimensionality, a linear projection onto a hyperplane is done. The hyperplane is chosen to **maximize the variance of the data after the projection** (there is an equivalent minimum reconstruction error formulation). This is done projecting onto the subspace generated by the eigenvectors of the covariance matrix with largest eigenvalues.\n",
    "\n",
    "The first principal component accounts for as much of the variability in the data as possible, and each succeeding component accounts for as much of the remaining variability as possible.\n",
    "\n",
    "**PCA is a dimensionality reduction or data compression method. The goal is dimension reduction and there is no guarantee that the dimensions are interpretable (a fact often not appreciated).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Algorithm\n",
    "\n",
    "Let's $X_i^{(n)}$ be the features matrix, $n=1,\\ldots,N$ the number of samples and $i = 1,\\ldots,D$ the number of features. We want to transform them into a lower dimensional space of dimension $M$, generating a reduced features matrix $Z_\\alpha^{(n)}$.\n",
    "\n",
    "**TRAIN PHASE**\n",
    "\n",
    "1- Calculate the sample covariance matrix\n",
    "$$\n",
    "\\Sigma_{ij} = \\frac{1}{N}\\sum_n (X^{(n)}-\\bar{X})_i (X^{(n)}-\\bar{X})_j \\;,\n",
    "$$\n",
    "where $\\bar{X}$ is the mean value of the feature vectors\n",
    "$$\n",
    "\\bar{X}_i = \\frac{1}{N}\\sum_n X_i^{(n)} \\;.\n",
    "$$\n",
    "\n",
    "2- Calculate the eigenvectors and eigenvalues of the sample covariance matrix. Sort them by eigenvalue.\n",
    "\n",
    "\n",
    "\n",
    "**PREDICT PHASE**\n",
    "\n",
    "*Normally, step 3 would be in the training phase, since it defines the algorithm, but once you have calculated the eigenvectors, you can easily project onto different dimension subspaces without calculating them again.*\n",
    "\n",
    "3- Select the $M$ eigenvectors $v_\\alpha$ with largest eigenvalues $\\lambda_\\alpha$, $\\alpha = 1,\\ldots,D$.\n",
    "\n",
    "4- Project onto the $M$ selected eigenvectors\n",
    "$$\n",
    "Z_\\alpha^{(n)} = \\sum_i \\left( v_{\\alpha i} X_i^{(n)}  \\right) \\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T19:45:54.370038Z",
     "start_time": "2019-04-07T19:45:54.352742Z"
    }
   },
   "outputs": [],
   "source": [
    "class PCA():\n",
    "    \"\"\"Principal component analysis (PCA)\n",
    "    Linear dimensionality reduction using Singular Value Decomposition of the\n",
    "    data to project it to a lower dimensional space.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int, float\n",
    "        Number of components to keep.\n",
    "    \n",
    "  \n",
    "    Attributes\n",
    "    ----------\n",
    "    dim_orig : int\n",
    "        Dimension of the original feature space.\n",
    "    eigenvectors : np.array, shape (dim_orig, dim_orig)\n",
    "        Principal axes in feature space, representing the directions of\n",
    "        maximum variance in the data. The components are sorted by decreasing\n",
    "        eigenvalue (explained variance).\n",
    "        indices: ($i$, $\\alpha$)\n",
    "    eigenvalues : np.array, shape (dim_orig,)\n",
    "        The eigenvalues corresponding to each of the eigenvectors.\n",
    "    explained_variance_cummulative : np.array, shape (dim_orig,)\n",
    "        Percentage of variance explained given for arbitrary n_components.\n",
    "    explained_variance : np.array, shape (dim_orig,)\n",
    "        Percentage of variance explained for a given n_components.\n",
    "\n",
    "    n_components : int\n",
    "        The estimated number of components.\n",
    " \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dim_orig = None\n",
    "        self.n_components = None\n",
    "        self.eigenvalues = None\n",
    "        self.eigenvectors = None\n",
    "        self.explained_variance_cummulative = None\n",
    "        self.explained_variance_percentage = None\n",
    "        return None\n",
    "    def train(self,X):\n",
    "        self.dim_orig = X.shape[1]\n",
    "        assert X.ndim == 2, 'X does not have ndim 2'\n",
    "        \n",
    "        # TODO: Calculate the covariance matrix of the sample\n",
    "        \n",
    "        cov = \n",
    "\n",
    "        assert cov.shape == (X.shape[1],)*2\n",
    "        assert (cov == cov.T).all()\n",
    "        # TODO: Calculate the eigencalues and eigenvectors. Hint: look in np.linalg\n",
    "        # Remember to save them in decreasing order.\n",
    "\n",
    "        self.eigenvalues = \n",
    "        self.eigenvectors = \n",
    "        \n",
    "        # TODO: Calculate explained_variance_cummulative\n",
    "        self.explained_variance_cummulative = \n",
    "        \n",
    "        return self\n",
    "    def apply(self,X,n_components):\n",
    "        assert type(n_components) is int, 'n_componets is not of type int'\n",
    "        assert n_components <= self.dim_orig,'n_componets cannot be greater than the number of features'\n",
    "        self.n_components = n_components\n",
    "        assert X.ndim == 2, 'X does not have ndim 2'\n",
    "        assert self.dim_orig == X.shape[1], 'input X does not have the same number of features as the set used for training'\n",
    "        self.explained_variance_percentage = self.explained_variance_cummulative[n_components-1]\n",
    "        \n",
    "        \n",
    "        # TODO: Project onto the n_components eigenvectros with largest eigenvalues\n",
    "        X_proj = \n",
    "        \n",
    "        \n",
    "        assert X_proj.shape == (X.shape[0],n_components)\n",
    "        return X_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T19:28:49.291834Z",
     "start_time": "2019-04-07T19:28:49.268342Z"
    }
   },
   "outputs": [],
   "source": [
    "X,Y = sklearn.datasets.load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T19:28:50.009068Z",
     "start_time": "2019-04-07T19:28:50.001732Z"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T19:59:48.390756Z",
     "start_time": "2019-04-07T19:59:48.305234Z"
    }
   },
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "pca = PCA().train(X)\n",
    "X_pca = pca.apply(X,n_components)\n",
    "print('Explained variance with',n_components,'principal components:',100*pca.explained_variance_percentage,'%')\n",
    "print('X_pca.shape :',X_pca.shape)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x= X_pca[:,0],y=X_pca[:,1],hue=Y, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the selection of the features was highly determined by the scales of the variables. The features with larger scales are prone to have larger variances, and dominate in the PCA algorithm. This is why is highly recommended to normalize the features beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T19:59:39.811946Z",
     "start_time": "2019-04-07T19:59:39.703652Z"
    }
   },
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "# Normalize with zero mean and standard deviation equal to one in each feature\n",
    "X_norm = (X - np.mean(X,axis=0))/np.std(X,axis=0)\n",
    "pca = PCA().train(X_norm)\n",
    "X__norm_pca = pca.apply(X_norm,n_components)\n",
    "print('Explained variance with',n_components,'principal components:',100*pca.explained_variance_percentage,'%')\n",
    "print('X_pca.shape :',X__norm_pca.shape)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x= X__norm_pca[:,0],y=X__norm_pca[:,1],hue=Y, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Whitening or Sphereing\n",
    "\n",
    "As we saw in previous lectures, data is usually preprocessed normalizing the features (zero mean, unit variance, scaling between zero and one, ...). \n",
    "\n",
    "Instead of using PCA for dimensionality reduction, it can be applied for data preprocessing. Use the previously implemented PCA algorithm to define a function which apply whitening, i.e., it returns data with zero mean and identity matrix covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T20:00:55.852817Z",
     "start_time": "2019-04-07T20:00:55.846349Z"
    }
   },
   "outputs": [],
   "source": [
    "def spherizer(X):\n",
    "\n",
    "    return X_sphe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spherizer applied\n",
    "\n",
    "In order to can visualize what is going on, we are going to choose only the first 5 features of the dataset, and see how spherizing can help in a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T20:11:46.666501Z",
     "start_time": "2019-04-07T20:11:46.660741Z"
    }
   },
   "outputs": [],
   "source": [
    "X6 = X[:,0:6]\n",
    "X_sphe = spherizer(X6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T20:11:50.411770Z",
     "start_time": "2019-04-07T20:11:47.429848Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X6)\n",
    "df['Y']=pd.Series(Y)\n",
    "g = sns.pairplot(df,vars=[0,1,3,4,5],hue='Y')\n",
    "g.fig.set_size_inches(8,8)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T20:11:57.081086Z",
     "start_time": "2019-04-07T20:11:54.369243Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sphe = pd.DataFrame(X_sphe)\n",
    "df_sphe['Y']=pd.Series(Y)\n",
    "g = sns.pairplot(df_sphe,vars=[0,1,3,4,5],hue='Y')\n",
    "g.fig.set_size_inches(8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Compare classifiers with and without whitening.\n",
    "\n",
    "Note: You cannot learn the parameters of the whitening (as the mean, the rotation and the scalings), or any other normalization, from the validation or test sets. They have to be learned from the training set only, and apply the same transformation to the validation and test sets in order to apply the algorithms to them."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.2",
    "jupytext_version": "1.0.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
